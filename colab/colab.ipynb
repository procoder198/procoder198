{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/procoder198/procoder198/blob/main/colab/colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 1 -- press play if you are on phone{ display-mode: \"form\" }\n",
        "%%html\n",
        "<audio autoplay=\"\" src=\"https://raw.githubusercontent.com/KoboldAI/KoboldAI-Client/main/colab/silence.m4a\" loop controls>"
      ],
      "metadata": {
        "id": "2H92KghtkL1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsUhAZBU-dHc",
        "cellView": "form",
        "outputId": "a752c4a9-8960-478d-c8d0-7dc6636446bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to TavernAI with Koboldcpp\n",
            "Colab notebook written by doctord98 with added api support by vrihatgan\n",
            "Please wait while I setup everything for you\n",
            "downloading model\n"
          ]
        }
      ],
      "source": [
        "#@title Step 2 -- press play this to run TavernAI with koboldcpp\n",
        "\n",
        "Model = \"https://huggingface.co/bartowski/magnum-v4-12b-GGUF/resolve/main/magnum-v4-12b-Q5_K_M.gguf\"\n",
        "use_drive_for_chat_backup = False #@param {type:\"boolean\"}\n",
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "import shutil\n",
        "import threading\n",
        "import torch\n",
        "import sys\n",
        "import requests\n",
        "if not torch.cuda.is_available():\n",
        "    from IPython.display import display, Markdown\n",
        "    display(Markdown(\"<h1>NO free gpu is avilable for you at the moment</h1>\"))\n",
        "    sys.exit()\n",
        "from google.colab import drive\n",
        "os.chdir(\"/content\")\n",
        "print(\"Welcome to TavernAI with Koboldcpp\")\n",
        "print(\"Colab notebook written by doctord98 with added api support by vrihatgan\")\n",
        "print(\"Please wait while I setup everything for you\")\n",
        "if use_drive_for_chat_backup:\n",
        "  drive.mount('/content/drive')\n",
        "subprocess.call(\"git clone https://github.com/vrihatgan/TavernAI.git\", shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "src1 = '/content/TavernAI/public/characters'\n",
        "des1 = '/content/drive/MyDrive/TavernAI/characters'\n",
        "src2 = '/content/TavernAI/public/chats'\n",
        "des2 = '/content/drive/MyDrive/TavernAI/chats'\n",
        "Layers = 99\n",
        "ContextSize = 6144\n",
        "subprocess.run(\"wget -O dlfile.tmp https://kcpplinux.concedo.workers.dev && mv dlfile.tmp koboldcpp_linux\", shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "subprocess.run(\"chmod +x ./koboldcpp_linux\", shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "subprocess.run(\"apt install aria2 -y\", shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "print(\"downloading model\")\n",
        "subprocess.run(f\"aria2c -x 10 -o model.gguf --summary-interval=5 --download-result=default --allow-overwrite=true --file-allocation=none {Model}\", shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "print(\"downloading complete\")\n",
        "print(\"now loading model\")\n",
        "command = f\"./koboldcpp_linux model.gguf --usecublas 0 mmq --multiuser --gpulayers {Layers} --contextsize {ContextSize} --quiet\"\n",
        "subprocess.Popen(command, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "flag_file = \"backup_flag.txt\"\n",
        "if use_drive_for_chat_backup:\n",
        "    if os.path.exists(flag_file):\n",
        "        print(\"The code has already been executed.\")\n",
        "    else:\n",
        "\n",
        "      if not os.path.exists(des1):\n",
        "        os.makedirs(des1)\n",
        "        shutil.copytree(src1, des1, dirs_exist_ok=True)\n",
        "      else:\n",
        "        !rm -rf /content/TavernAI/public/characters/*\n",
        "        shutil.copytree(des1, src1, dirs_exist_ok=True)\n",
        "      if not os.path.exists(des2):\n",
        "        os.makedirs(des2)\n",
        "        shutil.copytree(src2, des2, dirs_exist_ok=True)\n",
        "      else:\n",
        "        !rm -rf /content/TavernAI/public/chats/*\n",
        "        shutil.copytree(des2, src2, dirs_exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "      def backup_thread():\n",
        "        while True:\n",
        "          try:\n",
        "            shutil.copytree(src1, des1, dirs_exist_ok=True)\n",
        "            shutil.copytree(src2, des2, dirs_exist_ok=True)\n",
        "          except Exception as e:\n",
        "            pass\n",
        "\n",
        "          time.sleep(5)\n",
        "\n",
        "      backup_thread = threading.Thread(target=backup_thread)\n",
        "      backup_thread.start()\n",
        "\n",
        "      with open(flag_file, \"w\") as f:\n",
        "        f.write(\"you don't need to run this code again just run the code below if link not working\")\n",
        "\n",
        "subprocess.call(\"npm install -g npm@8.19.3\", shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "os.chdir(\"/content/TavernAI\")\n",
        "subprocess.call(\"npm install\", shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "os.chdir('/content/')\n",
        "if not os.path.isfile('cloudflared-linux-amd64'):\n",
        "    os.system('wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64')\n",
        "    os.chmod('cloudflared-linux-amd64', 0o777)\n",
        "\n",
        "os.system('nohup ./cloudflared-linux-amd64 tunnel --url http://127.0.0.1:8000 --metrics 127.0.0.1:31337 > cf.txt 2>&1 &')\n",
        "\n",
        "time.sleep(10)\n",
        "\n",
        "def is_service_up(url, timeout=5):\n",
        "    try:\n",
        "        response = requests.get(url, timeout=timeout)\n",
        "        return response.status_code == 200\n",
        "    except requests.ConnectionError:\n",
        "        return False\n",
        "\n",
        "metrics_url = 'http://127.0.0.1:31337/metrics'\n",
        "if is_service_up(metrics_url):\n",
        "    try:\n",
        "        scrape = requests.get(metrics_url).text\n",
        "        needle = scrape.partition('cloudflared_tunnel_user_hostnames_counts{userHostname=\"')[2].split('\"} 1')[0]\n",
        "\n",
        "        if needle:\n",
        "            with open('cloudflare.log', 'w') as log:\n",
        "                log.write(\"CLOUDFLARE PROVIDES!\" + needle)\n",
        "        print(needle)\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Error fetching metrics: {e}\")\n",
        "else:\n",
        "    print(\"Metrics service is not available.\")\n",
        "\n",
        "os.chdir(\"/content/TavernAI\")\n",
        "!nohup node server.js >/dev/null 2>&1\n"
      ]
    }
  ]
}